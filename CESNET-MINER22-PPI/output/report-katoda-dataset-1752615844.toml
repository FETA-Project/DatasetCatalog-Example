[collection_workflow]
data_collection_tool = """  """
data_collection_year = """  """
feature_extraction_tool_info = """ Tool that converts dataset to feature dataset. If any. """
feature_extraction_tool = """  """
feature_extraction_tool_description = """  """
capture_config_parameters_info = """ specific parameters that were used to capture dataset or feature dataset """
capture_config_parameters = """  """
real_dataset_info = """ Source of the dataset. E.g., real environment, testbed or generated. """
real_dataset = """  """
annotation_info = """ Description of the dataset annotation. E.g., manual, automatic """
annotation = """  """

[generic_info]
classes = """ 2 """
features = """ 90 """
f1-score_info = """ F1-score calculated based on NDVM tool [https://github.com/soukudom/NDVM] """
f1-score = """ 0.985023903078569 """
performance_metric_info = """ Perfomance metric defined by the author. Please define full specification e.g., F1-weighted """
performance_metric_name = """  """
performance_metric_value = """  """
label_info = """ Name of the field with label. In case this is unsupervised dataset, type None """
label = """  """
known_issues_info = """ Description of indentified issues in the dataset """
known_issues = """   """
key_observations_info = """ List of known errors, drifts, limits, ... of the dataset """
key_observations = """  *  The dataset has 0 small classes, 302 duplicated samples, 0 nan value """
dataset_organization_info = """ Structure of the dataset. E.g., per day, per capture, per device """
dataset_organization = """  """
dataset_organization_description_info = """ Description of the content of the organization. Is there any metadata? """
dataset_organization_description = """   """
dataset_documentation_info = """ How to get started with the dataset. Ideally add example notebook. """
dataset_documentation = """   """
used_dataset_info = """ Script to get dataset for provided analysis """
used_dataset = """ get-dataset.py """
dataset_application_info = """ Where the dataset has been already applied. """
dataset_application = """   """
per_class_data = """{
  "Miner": "2003",
  "Other": "1996"
}"""
per_feature_data = """{
  "IPT_1": "",
  "IPT_2": "",
  "IPT_3": "",
  "IPT_4": "",
  "IPT_5": "",
  "IPT_6": "",
  "IPT_7": "",
  "IPT_8": "",
  "IPT_9": "",
  "IPT_10": "",
  "IPT_11": "",
  "IPT_12": "",
  "IPT_13": "",
  "IPT_14": "",
  "IPT_15": "",
  "IPT_16": "",
  "IPT_17": "",
  "IPT_18": "",
  "IPT_19": "",
  "IPT_20": "",
  "IPT_21": "",
  "IPT_22": "",
  "IPT_23": "",
  "IPT_24": "",
  "IPT_25": "",
  "IPT_26": "",
  "IPT_27": "",
  "IPT_28": "",
  "IPT_29": "",
  "IPT_30": "",
  "DIR_1": "",
  "DIR_2": "",
  "DIR_3": "",
  "DIR_4": "",
  "DIR_5": "",
  "DIR_6": "",
  "DIR_7": "",
  "DIR_8": "",
  "DIR_9": "",
  "DIR_10": "",
  "DIR_11": "",
  "DIR_12": "",
  "DIR_13": "",
  "DIR_14": "",
  "DIR_15": "",
  "DIR_16": "",
  "DIR_17": "",
  "DIR_18": "",
  "DIR_19": "",
  "DIR_20": "",
  "DIR_21": "",
  "DIR_22": "",
  "DIR_23": "",
  "DIR_24": "",
  "DIR_25": "",
  "DIR_26": "",
  "DIR_27": "",
  "DIR_28": "",
  "DIR_29": "",
  "DIR_30": "",
  "SIZE_1": "",
  "SIZE_2": "",
  "SIZE_3": "",
  "SIZE_4": "",
  "SIZE_5": "",
  "SIZE_6": "",
  "SIZE_7": "",
  "SIZE_8": "",
  "SIZE_9": "",
  "SIZE_10": "",
  "SIZE_11": "",
  "SIZE_12": "",
  "SIZE_13": "",
  "SIZE_14": "",
  "SIZE_15": "",
  "SIZE_16": "",
  "SIZE_17": "",
  "SIZE_18": "",
  "SIZE_19": "",
  "SIZE_20": "",
  "SIZE_21": "",
  "SIZE_22": "",
  "SIZE_23": "",
  "SIZE_24": "",
  "SIZE_25": "",
  "SIZE_26": "",
  "SIZE_27": "",
  "SIZE_28": "",
  "SIZE_29": "",
  "SIZE_30": "",
  "label": ""
}"""

[dataset_drift_analysis]

[advanced_metrics]
description = """  """
perqoda_permutation_slope = """ 0.42711379818909045 """
p_value_status = """ Good """
redundancy = """ 0.4765625 """
similarity = """ 2.646805329049232 """
advanced_metrics_workflow = """ dataset-metrics.json """

[dataset_comparison]
description = """ ML model comparison for this dataset """
use_case = """   """
similar_dataset = """   """
